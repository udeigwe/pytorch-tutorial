{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSqy175rlSlIqVzpduYNNn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udeigwe/pytorch_tutorial/blob/main/pytorch_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtLVMMeSW91I"
      },
      "source": [
        "### AUTOGRAD: AUTOMATIC DIFFERENTIATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKv-xaKiW4_E",
        "outputId": "d044a138-e0b0-49b3-d5e1-0a79b5cbea0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "#Create a tensor and set requires_grad=True \n",
        "# to track computation with it\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3XoSKjwYh6P",
        "outputId": "c96fdc33-326d-4bee-f3e4-e31ea3a0fb91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Do a tensor operation:\n",
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX7yHJPKY22G",
        "outputId": "6a345c0d-5ec4-4ac4-b188-88d18c00828a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# y was created as a result of an operation, so it has a grad_fn\n",
        "print(y.grad_fn)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7ff2203d6208>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9kAJarZZBjz",
        "outputId": "1dce71da-0053-4b51-f23a-9455d958b077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Do more operations on y\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "print(z, out)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZfis3IEZLlk",
        "outputId": "273d375d-f4b0-46ab-bef7-3cf5faadc8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# .requires_grad_( ... ) changes an existing Tensor’s requires_grad flag in-place. \n",
        "# The input flag defaults to False if not given.\n",
        "\n",
        "a = torch.randn(2, 2)\n",
        "a = ((a * 3) / (a - 1))\n",
        "print(a.requires_grad)\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x7ff1d64dbf28>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXGxzClwqhl4"
      },
      "source": [
        "### Gradients\n",
        "Let’s backprop now. Because out contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE67xGr1bC0b",
        "outputId": "b78e44c4-401a-4438-ec61-fc0997980324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "out.backward()\n",
        "\n",
        "#Print gradients d(out)/dx\n",
        "print(x.grad)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNrvxMUQrRGm",
        "outputId": "c561d5ef-0e7b-41a8-d46e-8cf1c1b8aa4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Now let’s take a look at an example of vector-Jacobian product:\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "#print(x)\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    #print(y.data.norm())\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.8672, -0.2002, -0.4483], requires_grad=True)\n",
            "tensor(1.9932)\n",
            "tensor(3.9863)\n",
            "tensor(7.9727)\n",
            "tensor(15.9453)\n",
            "tensor(31.8907)\n",
            "tensor(63.7814)\n",
            "tensor(127.5628)\n",
            "tensor(255.1256)\n",
            "tensor(510.2511)\n",
            "tensor([ 888.0473, -205.0088, -459.0949], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ4kXsSIvETo",
        "outputId": "01d83133-a46e-4c4a-9907-e05abd8e479b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Now in this case y is no longer a scalar. \n",
        "# torch.autograd could not compute the full Jacobian directly, \n",
        "# but if we just want the vector-Jacobian product, \n",
        "# simply pass the vector to backward as argument:\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(v)\n",
        "print(x.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGgdsgrQyFs8",
        "outputId": "c43e608d-8bf8-48f4-ddbc-1cf3337e1169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# You can also stop autograd from tracking history on Tensors \n",
        "# with .requires_grad=True either by wrapping the code block \n",
        "# in with torch.no_grad():\n",
        "\n",
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi4b9BHM05wi",
        "outputId": "7b8f7efe-4b25-42c4-d3a0-ea34f3f457c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Or by using .detach() to get a new Tensor with the \n",
        "# same content but that does not require gradients:\n",
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh61cDF41YRi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}